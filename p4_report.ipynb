{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing, Health, and Happiness â€“ Milestone P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure reproduction: Table 4\n",
    "\n",
    "The goal of this milestone is to reproduce Table 4 of the paper _Housing, Health, and Happiness_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Understanding what we'll need\n",
    "\n",
    "We started this project by reading section V of the paper to understand what part of the data (which columns) will be useful for reproducing the figure.\n",
    "\n",
    "The data related to the dependant variables can be fount in the following columns:\n",
    "  + Share of rooms with cement floors (`S_shcementfloor`)\n",
    "  + Cement floor in kitchen (`S_cementfloorkit`)\n",
    "  + Cement floor in dining room (`S_cementfloordin`)\n",
    "  + Cement floor in bathroom (`S_cementfloorbat`)\n",
    "  + Cement floor in bedroom (`S_cementfloorbed`)\n",
    "\n",
    "Control and treatment groups are identified by `dpisofirme` (control = 0, treatment = 1).\n",
    "\n",
    "Model 1 has no control variables.\n",
    "\n",
    "Model 2 has (25 - 1) control variables:\n",
    "  + demographic:\n",
    "    + Number of household members (`S_HHpeople`)\n",
    "    + (Number of rooms (`S_rooms`) -> This one is mentioned in the paper, but after looking at the STATA file, I noticed it was not used for the regression and decided to drop it)\n",
    "    + Head of household's years of schooling (`S_headeduc`)\n",
    "    + Spouse's years of schooling (`S_spouseeduc`)\n",
    "    + Head of household's age (`S_headage`)\n",
    "    + Spouse's age (`S_spouseage`)\n",
    "    + Proportion of Males 0-5yrs in household (`S_dem1`)\n",
    "    + Proportion of Males 6-17yrs in household (`S_dem2`)\n",
    "    + Proportion of Males 18-49yrs in household (`S_dem3`)\n",
    "    + Proportion of Males 50+yrs in household (`S_dem4`)\n",
    "    + Proportion of Females 0-5yrs in household (`S_dem5`)\n",
    "    + Proportion of Females 6-17yrs in household (`S_dem6`)\n",
    "    + Proportion of Females 18-49yrs in household (`S_dem7`)\n",
    "    + Proportion of Females 50+yrs in household (`S_dem8`)\n",
    "  + health:\n",
    "    + Household has animals on land (`S_hasanimals`)\n",
    "    + Animals allowed to enter the house (`S_animalsinside`)\n",
    "    + Water connection outside (`S_waterland`)\n",
    "    + Water connection inside the house (`S_waterhouse`)\n",
    "    + Electricity (`S_electricity`)\n",
    "    + Number of times respondent washed hands the day before (`S_washhands`)\n",
    "    + Uses garbage collection service (`S_garbage`)\n",
    "    \n",
    "Model 3 adds 4 control variables:\n",
    "  + Transfers per capita from government programs (`S_cashtransfers`)\n",
    "  + Household beneficiary of government milk supplement program (`S_milkprogram`)\n",
    "  + Household beneficiary of government food program (`S_foodprogram`)\n",
    "  + Household beneficiary of seguro popular (`S_seguropopular`)\n",
    "  \n",
    "All models use `idcluster` for clustering.\n",
    "\n",
    "Moreover, the paper mentioned dropping samples for which geographical data was available. Initially, I used that as a criteria for filtering the dataset. However, doing this gave worse results than not filtering at all. Thanks to a helpful comment on Zulip, I noticed a discreptancy between the number of samples I was supposed to have according to Table 1 (1362) and the number I had (1187). After looking around, I noticed geographical data was missing for the following 203 lines: (1788:1916), (2505:2527), (2576:2592), (2656:2661), (2755:2782). The last 28 lines seem to correspond to the 28 that were dropped in the paper, but the others seem to be due to some corruption of the dataset for some reason. Therefore, I decided to only drop the final 28 lines in order to use the same dataset as the paper, and I get much better results (which will be discussed in section 3) that way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and data loading\n",
    "\n",
    "The first step is to import the necessary libraries as well as loading the data, only keeping the necessary rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "\n",
    "DATA_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_t4 = {\n",
    "    \"treatment_var\": ['dpisofirme'],\n",
    "    \"clustering_var\": ['idcluster'],\n",
    "    \"demographic_control_vars\": ['S_HHpeople', 'S_headeduc', 'S_spouseeduc', 'S_headage', 'S_spouseage', 'S_dem1', 'S_dem2', 'S_dem3', 'S_dem4', 'S_dem5', 'S_dem6', 'S_dem7', 'S_dem8'],\n",
    "    \"health_control_vars\": ['S_hasanimals', 'S_animalsinside', 'S_waterland', 'S_waterhouse', 'S_electricity', 'S_washhands', 'S_garbage'],\n",
    "    \"model3_control_vars\": ['S_cashtransfers', 'S_milkprogram', 'S_foodprogram', 'S_seguropopular'],\n",
    "    \"dependant_vars\": ['S_shcementfloor', 'S_cementfloorkit', 'S_cementfloordin','S_cementfloorbat', 'S_cementfloorbed']\n",
    "}\n",
    "\n",
    "models_t4 = {\n",
    "    'model_1': [],\n",
    "    'model_2': vars_t4[\"demographic_control_vars\"] + vars_t4[\"health_control_vars\"],\n",
    "}\n",
    "models_t4['model_3'] = models_t4['model_2'] + vars_t4[\"model3_control_vars\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating missing values\n",
    "\n",
    "Missing values are replaced by 0 and a dummy variable indicating whether the value was missing is added (missing=1, present=0) for each variable containing missing values (others would only contain 0). The models are also updated to take the dummy variables into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter inplace is used to show explicitly that this function has side effects on df\n",
    "def generateMissingValues(df, models_, inplace):\n",
    "    models = copy.deepcopy(models_)\n",
    "    columns = models[\"model_3\"]\n",
    "    if(inplace == False):\n",
    "        raise ValueError(\"Parameter inplace has to be true\")\n",
    "    for col in columns:\n",
    "        if df[col].isnull().values.any():\n",
    "            df['dmiss_' + col] = df[col].apply(pd.isna).apply(float)\n",
    "            if col in models['model_2']:\n",
    "                models['model_2'].append('dmiss_' + col)\n",
    "            models['model_3'].append('dmiss_' + col)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression\n",
    "\n",
    "There are 2 steps to this part.\n",
    "\n",
    "The first is to compute the mean and standard deviation for the control group for each dependant variable. This is done using `mean()` and `std()` on the control DataFrame (i.e. `dpisofirme` = 0).\n",
    "\n",
    "The second part is to do a linear regression for each dependent variable once for each model. This is his done using 2 nested for loops (over models, then over dependent variables) and using `statsmodels`'s `OLS` with a cluster covariance estimator (`idcluster`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to convert p-value to stars like in the paper\n",
    "def to_stars(p):\n",
    "    if p < 0.01:\n",
    "        return \"***\"\n",
    "    elif p < 0.05:\n",
    "        return \"** \"\n",
    "    elif p < 0.1:\n",
    "        return \"*  \"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeResults(df, models, vs):\n",
    "    # Part 1: control\n",
    "    dependant_vars = vs[\"dependant_vars\"]\n",
    "    control = df[df.dpisofirme == 0][dependant_vars]\n",
    "    res = pd.DataFrame({\n",
    "        'control means': control.mean(),\n",
    "        'control std': control.std()\n",
    "    }, index=dependant_vars)\n",
    "    \n",
    "    # Part 2: linear regression\n",
    "    Y = df[dependant_vars]\n",
    "    \n",
    "    for k, v in models.items():\n",
    "        X = df[vs[\"treatment_var\"] + v]\n",
    "        X = sm.add_constant(X)\n",
    "        column = []\n",
    "        for label, y in Y.items():\n",
    "            regression = sm.OLS(y, X).fit(cov_type='cluster',\n",
    "                                          cov_kwds={'groups': df[vs[\"clustering_var\"]]})\n",
    "            coeff = regression.params['dpisofirme']\n",
    "            significance = to_stars(regression.pvalues['dpisofirme'])\n",
    "            column.append((coeff, regression.bse['dpisofirme'], significance, \n",
    "                           100 * coeff / res.loc[label]['control means']))\n",
    "        res[k] = column\n",
    "    \n",
    "    return pd.DataFrame(res, index=dependant_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Showing and discussing the results\n",
    "\n",
    "I display the DataFrame containing the results rounded to 3 decimals, then compute the difference (after rounding) with the results from the paper. \n",
    "\n",
    "The differences is fairly small (~10e-3 for the coeffs) for models 2 and 3. This may be due to the way I handled missing values in the control variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers as nb\n",
    "def round3(val):\n",
    "    if isinstance(val, nb.Number):\n",
    "        return round(val, 3)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        tpe = type(val)\n",
    "        return tpe(map(round3, val))\n",
    "\n",
    "def round_res(df, vs):\n",
    "    res = df.apply(round3)\n",
    "    res.index = vs[\"dependant_vars\"]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_res = pd.DataFrame({'control means': [0.728, 0.671, 0.709, 0.803, 0.668],\n",
    "                             'control std': [0.363, 0.470, 0.455, 0.398, 0.471],\n",
    "                             'model_1': [(0.202, 0.021, '***', 27.746),\n",
    "                                         (0.255, 0.025, '***', 37.936),\n",
    "                                         (0.210, 0.026, '***', 29.633),\n",
    "                                         (0.105, 0.022, '***', 13.071),\n",
    "                                         (0.238, 0.020, '***', 35.598)],\n",
    "                             'model_2': [(0.208, 0.019, '***', 28.512),\n",
    "                                         (0.260, 0.023, '***', 38.708),\n",
    "                                         (0.217, 0.025, '***', 30.588),\n",
    "                                         (0.113, 0.018, '***', 14.043),\n",
    "                                         (0.245, 0.021, '***', 36.735)],\n",
    "                             'model_3':[(0.210, 0.019, '***', 28.876),\n",
    "                                        (0.265, 0.023, '***', 39.440),\n",
    "                                        (0.221, 0.025, '***', 31.189),\n",
    "                                        (0.117, 0.018, '***', 14.536),\n",
    "                                        (0.245, 0.020, '***', 36.695)]},\n",
    "                            index = vars_t4[\"dependant_vars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "#x, y chars\n",
    "def star_compare(x, y):\n",
    "    values = {' ', '*'}\n",
    "    if(x not in values or y not in values):\n",
    "        return '/'\n",
    "    bx = x == '*'\n",
    "    by = y == '*'\n",
    "    res = [[' ', '-'], ['+', '*']]\n",
    "    return res[bx][by]\n",
    "\n",
    "def f(x, y):\n",
    "    if isinstance(x, tuple):\n",
    "        return tuple(map(f, x, y))\n",
    "    elif not (isinstance(x, str)):\n",
    "        return \"{:.2e}\".format(operator.sub(x, y))\n",
    "    else:\n",
    "        return \"\".join(map(star_compare, list(x),list(y)))\n",
    "    \n",
    "def diff(df1, df2):\n",
    "    comp = df1.copy()\n",
    "    for col in comp.columns:\n",
    "        comp[col] = list(map(f, comp[col], df2[col]))\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t4 = pd.read_stata(DATA_PATH + \"PisoFirme_AEJPol-20070024_household.dta\")[:2755]\n",
    "df_t4 = df_t4[vars_t4[\"treatment_var\"] + vars_t4[\"clustering_var\"] + models_t4['model_3'] + vars_t4[\"dependant_vars\"]]\n",
    "new_models_t4 = generateMissingValues(df_t4, models_t4, inplace=True)\n",
    "\n",
    "res = computeResults(df_t4, new_models_t4, vars_t4)\n",
    "\n",
    "rounded_res = round_res(res, vars_t4)\n",
    "display(rounded_res)\n",
    "\n",
    "display(expected_res)\n",
    "comp = diff(expected_res, rounded_res)\n",
    "display(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure reproduction: Table 5\n",
    "\n",
    "The goal of this milestone is to reproduce Table 5 of the paper _Housing, Health, and Happiness_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Understanding what we'll need\n",
    "\n",
    "We started this replication exercise by adapting our replication of Table 4. The main differences are:\n",
    "1. we use the `individual.dta` rather than the `household.dta` file;\n",
    "1. we use different dependant variables.\n",
    "\n",
    "Note that Table 5 only focuses on children under the age of 6, so we drop every row pertaining to a person older than 6 years old (see [Annex A](#annexA) for further justification).\n",
    "\n",
    "Using the explanation in section V of the paper as well as the STATA code, we identified the parts of the data which would be useful for reproducing the figure.\n",
    "\n",
    "The data related to the dependant variables can be found in the following columns:\n",
    "  + Parasite count (`S_parcount`)\n",
    "  + Diarrhea (`S_diarrhea`)\n",
    "  + Anemia (`S_anemia`)\n",
    "  + McArthur Communication Development Test score (`S_mccdts`)\n",
    "  + Picture Peabody Vocabulary Test percentile score (`S_pbdypct`)\n",
    "  + Height-for-age z-score (`S_haz`)\n",
    "  + Weight-for-height z-score (`S_whz`)\n",
    "\n",
    "Control and treatment groups are identified by `dpisofirme` (control = 0, treatment = 1).\n",
    "\n",
    "Model 1 has no control variables.\n",
    "\n",
    "Model 2 has 58 control variables:\n",
    "  + demographic:\n",
    "    + Number of household members (`S_HHpeople`)\n",
    "    + Number of rooms (`S_rooms`)\n",
    "    + Age (`S_age`)\n",
    "    + Male (`S_gender`) -> the README specifies that Male = 1, but the loaded dataframe contained the values `0.0` and `hombre`, so this was corrected to be `0` and `1`\n",
    "    + Mother of at least one child in household present (`S_childma`)\n",
    "    + Mother's age (if present) (`S_childmaage`)\n",
    "    + Mother's years of schooling (if present) (`S_childmaeduc`)\n",
    "    + Father of at least one child in household present (`S_childpa`)\n",
    "    + Father's age (if present) (`S_childpaage`)\n",
    "    + Father's years of schooling (if present) (`S_childpaeduc`)\n",
    "    + (Trimester * Gender) Dummy for children 0-5yrs (`dtriage*`) [48]\n",
    "  + health:\n",
    "    + Household has animals on land (`S_hasanimals`)\n",
    "    + Animals allowed to enter the house (`S_animalsinside`)\n",
    "    + Water connection outside (`S_waterland`)\n",
    "    + Water connection inside the house (`S_waterhouse`)\n",
    "    + Electricity (`S_electricity`)\n",
    "    + Number of times respondent washed hands the day before (`S_washhands`)\n",
    "    + Uses garbage collection service (`S_garbage`)\n",
    "    \n",
    "Model 3 adds 4 control variables:\n",
    "  + Transfers per capita from government programs (`S_cashtransfers`)\n",
    "  + Household beneficiary of government milk supplement program (`S_milkprogram`)\n",
    "  + Household beneficiary of government food program (`S_foodprogram`)\n",
    "  + Household beneficiary of seguro popular (`S_seguropopular`)\n",
    "  \n",
    "All models use `idcluster` for clustering.\n",
    "\n",
    "Moreover, the paper mentioned dropping samples for which geographical data was unavailable. In the first replication, we noticed that there were multiple clusters of rows for which geographical data was missing, but that we got the correct number of datapoints and much better results by only dropping the final cluster, so we tried applying the same method here. Since the paper never mentions how many individuals' data they used, we used the data provided in Table 1 for each dependant variable as a point of comparison. The list of rows with missing data can be found in [Annex B](#annexB), and the check of the number of datapoints after dropping the final cluster is done in [Annex C](#annexC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and data loading\n",
    "\n",
    "The first step is to import the necessary libraries as well as loading the data, only keeping the necessary rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "DATA_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_stata(DATA_PATH + \"PisoFirme_AEJPol-20070024_individual.dta\")\n",
    "children_df = original_df[original_df.S_age < 6].reset_index(drop=True)\n",
    "children_df['S_gender'] = children_df['S_gender'].apply(lambda x: x == 'hombre').astype(int)\n",
    "df = children_df[:4052] # Magic number! See Annex B!\n",
    "\n",
    "treatment_var =  ['dpisofirme']\n",
    "clustering_var = ['idcluster']\n",
    "demographic_control_vars_1 = ['S_HHpeople', 'S_rooms', 'S_age', 'S_gender', 'S_childma', 'S_childmaage',\n",
    "                            'S_childmaeduc', 'S_childpa', 'S_childpaage', 'S_childpaeduc']\n",
    "demographic_control_vars_2 = [x for x in df.columns if 'dtriage' in x]\n",
    "health_control_vars = ['S_hasanimals', 'S_animalsinside', 'S_waterland', 'S_waterhouse', \n",
    "                  'S_electricity', 'S_washhands', 'S_garbage']\n",
    "model3_control_vars = ['S_cashtransfers', 'S_milkprogram', 'S_foodprogram', 'S_seguropopular']\n",
    "dependant_vars = ['S_parcount', 'S_diarrhea', 'S_anemia', 'S_mccdts', 'S_pbdypct', 'S_haz', 'S_whz']\n",
    "models = {\n",
    "    'model_1': [],\n",
    "    'model_2': demographic_control_vars_1 + demographic_control_vars_2 + health_control_vars,\n",
    "    'model_3': demographic_control_vars_1 + demographic_control_vars_2 + health_control_vars + model3_control_vars\n",
    "}\n",
    "\n",
    "df = df[treatment_var + clustering_var + dependant_vars + models['model_3'] + ['coord_x']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating missing values\n",
    "\n",
    "Missing values are replaced by 0 and a dummy variable indicating whether the value was missing is added (missing=1, present=0) for each independant variable containing missing values (others would only contain 0). The models are also updated to take the dummy variables into account.\n",
    "\n",
    "Note: in the STATA file the paper's authors do not check for missing values in `dtriage` colums. Here, we confirm that there is no missing values in these columns, so we do not need to manually exclude them from the `values` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = models[\"model_3\"]\n",
    "values = dict(zip(columns, [0] * len(columns)))\n",
    "\n",
    "print('Columns with missing values:')\n",
    "for col in columns:\n",
    "    if df[col].isnull().values.any():\n",
    "        print(col)\n",
    "        df['dmiss_' + col] = df[col].apply(pd.isna).apply(int)\n",
    "        if col in models['model_2']:\n",
    "            models['model_2'].append('dmiss_' + col)\n",
    "        models['model_3'].append('dmiss_' + col)\n",
    "df.fillna(values, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression\n",
    "\n",
    "There are 2 steps to this part.\n",
    "\n",
    "The first is to compute the mean and standard deviation for the control group for each dependant variable. This is done using `mean()` and `std()` on the control DataFrame (i.e. `dpisofirme` = 0).\n",
    "\n",
    "The second part is to do a linear regression for each dependent variable once for each model. This is his done using 2 nested for loops (over models, then over dependent variables) and using `statsmodels`'s `OLS` with a cluster covariance estimator (`idcluster`). Since we have missing values for certain rows in the dependant variables, we drop them for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: control\n",
    "control = df[df.dpisofirme == 0][dependant_vars]\n",
    "results = pd.DataFrame({\n",
    "    'control means': control.mean(),\n",
    "    'control std': control.std()\n",
    "}, index=dependant_vars)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to convert p-value to stars like in the paper\n",
    "def to_stars(p):\n",
    "    if p < 0.01:\n",
    "        return \"***\"\n",
    "    elif p < 0.05:\n",
    "        return \"**\"\n",
    "    elif p < 0.1:\n",
    "        return \"*\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: linear regression\n",
    "Y = df[dependant_vars]\n",
    "\n",
    "for k, v in models.items():\n",
    "    X = df[treatment_var + v]\n",
    "    X = sm.add_constant(X)\n",
    "    column = []\n",
    "    for label, y in Y.items():\n",
    "        regression = sm.OLS(y, X, missing='drop').fit(cov_type='cluster',\n",
    "                                      cov_kwds={'groups': df.dropna(subset=[label])[clustering_var]})\n",
    "        coeff = regression.params['dpisofirme']\n",
    "        significance = to_stars(regression.pvalues['dpisofirme'])\n",
    "        column.append((coeff, regression.bse['dpisofirme'], significance,\n",
    "                       100 * coeff / results.loc[label]['control means']))\n",
    "    results[k] = column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Showing and discussing the results\n",
    "\n",
    "We display the DataFrame containing the results rounded to 3 decimals, then compute the difference (after rounding) with the results from the paper. \n",
    "\n",
    "The differences is fairly small (~10e-3 for the coeffs) for models 2 and 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers as nb\n",
    "def round3(val):\n",
    "    if isinstance(val, nb.Number):\n",
    "        return round(val, 3)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        tpe = type(val)\n",
    "        return tpe(map(round3, val))\n",
    "\n",
    "def round_res(df):\n",
    "    res = df.apply(round3)\n",
    "    res.index = dependant_vars\n",
    "    # print(res)\n",
    "    return res # printing is somehow prettier this way \n",
    "round_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_res = pd.DataFrame({'control means': [0.333, 0.142, 0.426, 13.354, 30.656, -0.605, 0.125],\n",
    "                             'control std': [0.673, 0.349, 0.495, 18.952, 24.864, 1.104, 1.133],\n",
    "                             'model_1': [(-0.065, 0.032, '**', -19.545),\n",
    "                                         (-0.018, 0.009, '*', -12.819),\n",
    "                                         (-0.085, 0.028, '***', -20.059), \n",
    "                                         (4.031, 1.650, '**', 30.182), \n",
    "                                         (2.668, 1.689,'' , 8.702), \n",
    "                                         (0.007, 0.043, '', -1.161), \n",
    "                                         (0.002, 0.034, '', 1.790)],\n",
    "                             'model_2': [(-0.064, 0.031, '**', -19.345),\n",
    "                                         (-0.020, 0.009, '**', -13.834),\n",
    "                                         (-0.081, 0.027, '***', -18.908),\n",
    "                                         (5.652, 1.642, '***', 42.325),\n",
    "                                         (3.206, 1.430, '**', 10.460),\n",
    "                                         (0.002, 0.038, '',0.279),\n",
    "                                         (-0.005, 0.036, '', -4.119)],\n",
    "                             'model_3':[(-0.064, 0.032, '**', -19.198),\n",
    "                                         (-0.018, 0.009, '*', -12.803),\n",
    "                                         (-0.083, 0.027, '***', -19.388),\n",
    "                                         (5.557, 1.641, '***', 41.609),\n",
    "                                         (3.083, 1.410, '**', 10.058),\n",
    "                                         (-0.002, 0.039, '', -0.323),\n",
    "                                         (-0.011, 0.037, '', -8.727)]},\n",
    "                            index = dependant_vars)\n",
    "expected_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def compare_results(x, y):\n",
    "    if isinstance(x, tuple):\n",
    "        return tuple(map(compare_results, x, y))\n",
    "    elif not (isinstance(x, str)):\n",
    "        return \"{:.2e}\".format(operator.sub(x, y))\n",
    "    else:\n",
    "        return x == y\n",
    "comp = expected_res.copy()\n",
    "res = round_res(results)\n",
    "for col in comp.columns:\n",
    "    comp[col] = list(map(compare_results, comp[col], res[col]))\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexA\"></span>\n",
    "### Annex A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = []\n",
    "olderthan6_df = original_df.drop(children_df.index)[dependant_vars]\n",
    "print('Checking if all dependant variables are `NaN` for individuals older than 6...')\n",
    "if (olderthan6_df.apply(pd.isna).values.all()):\n",
    "    s = \"\"\n",
    "else:\n",
    "    s = \"not\"\n",
    "print(f'We can{s} drop all aforementioned rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexB\"></span>\n",
    "### Annex B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = children_df.loc[pd.isna(children_df['coord_x'])].index\n",
    "ranges = []\n",
    "low, up = nans[0], nans[1]\n",
    "for i in range(len(nans) - 1):\n",
    "    up = nans[i]\n",
    "    if (up + 1 != nans[i+1] or i+1 == len(nans) - 1):\n",
    "        if (i+1 == len(nans) - 1):\n",
    "            up = nans[i+1]\n",
    "        ranges.append((low, up))\n",
    "        low = nans[i+1]\n",
    "ranges_str = \"\"\n",
    "for (l, r) in ranges:\n",
    "    ranges_str += '[' + str(l) + ', ' +  str(r) + ']\\n'\n",
    "print(f'Intervals of rows with missing geographical data:\\n{ranges_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexC\"></span>\n",
    "### Annex C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_amount = pd.DataFrame({'treat_expected': [1528, 1930, 1768, 291, 757, 1865, 1881],\n",
    "                                'cont_expected' : [1566, 2105, 1951, 302, 817, 2053, 2058]},\n",
    "                              index=dependant_vars)\n",
    "treat, cont, drop_t, drop_c = [], [], [], []\n",
    "treat_df = df[df.dpisofirme == 1]\n",
    "cont_df = df[df.dpisofirme == 0]\n",
    "\n",
    "for col in dependant_vars:\n",
    "    dt = len(treat_df.loc[pd.isna(treat_df[col])])\n",
    "    treat.append(len(treat_df) - dt)\n",
    "    drop_t.append(dt)\n",
    "    dc = len(cont_df.loc[pd.isna(cont_df[col])])\n",
    "    cont.append(len(cont_df) - dc)\n",
    "    drop_c.append(dc)\n",
    "expected_amount['treat'] = treat\n",
    "expected_amount['cont'] = cont\n",
    "expected_amount['dropped_treat'] = drop_t\n",
    "expected_amount['dropped_cont'] = drop_c\n",
    "expected_amount['dropped_tot'] = expected_amount['dropped_treat'] + expected_amount['dropped_cont']\n",
    "expected_amount['delta_t'] = expected_amount.treat_expected - expected_amount.treat\n",
    "expected_amount['delta_c'] = expected_amount.cont_expected - expected_amount.cont\n",
    "expected_amount['tot'] = expected_amount.treat + expected_amount.cont\n",
    "expected_amount"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
