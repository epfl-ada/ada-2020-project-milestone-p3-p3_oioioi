{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing, Health, and Happiness â€“ Milestone P4\n",
    "\n",
    "This milestone aims to provide an extension to the paper _Housing, Health, and Happiness_. Spcifically, we would like to make an additional matching on the household level in order to further confirm the conclusions of the authors, or to nuance them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numbers as nb\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from copy import deepcopy\n",
    "from math import inf\n",
    "\n",
    "DATA_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularised figure reproduction\n",
    "\n",
    "Here, we define modular functions for replicating the original tables from the paper, and to compare results to the expected values.\n",
    "These functions will be reused for generating new tables from the matched datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Defining common values\n",
    "\n",
    "Since the treatment variable and the clustering variable are the same across all datasets, we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ = {\n",
    "    \"treatment_var\": ['dpisofirme'],\n",
    "    \"clustering_var\": ['idcluster'],\n",
    "    \"demographic_control_vars\": [],\n",
    "    \"health_control_vars\": ['S_hasanimals', 'S_animalsinside', 'S_waterland', 'S_waterhouse',\n",
    "                            'S_electricity', 'S_washhands', 'S_garbage'],\n",
    "    \"model3_control_vars\": ['S_cashtransfers', 'S_milkprogram', 'S_foodprogram', 'S_seguropopular'],\n",
    "    \"dependant_vars\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(vars_):\n",
    "    models = {\n",
    "        'model_1': [],\n",
    "        'model_2': vars_[\"demographic_control_vars\"] + vars_[\"health_control_vars\"],\n",
    "    }\n",
    "    models['model_3'] = models['model_2'] + vars_[\"model3_control_vars\"]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating missing values\n",
    "\n",
    "Missing values in columns containing the independent variables are replaced by 0 and a dummy variable indicating whether the value was missing is added (missing=1, present=0) for each variable containing missing values (others would only contain 0). An updated model taking the dummy variables into account is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter inplace is used to show explicitly that this function has side effects on df\n",
    "def generate_missing_values(df, models_, inplace=True):\n",
    "    models = deepcopy(models_)\n",
    "    columns = models[\"model_3\"]\n",
    "    if not inplace:\n",
    "        raise ValueError(\"Parameter inplace has to be true\")\n",
    "    for col_name in columns:\n",
    "        if df[col_name].isnull().values.any():\n",
    "            new_col = 'dmiss_' + col_name\n",
    "            df[new_col] = df[col_name].apply(pd.isna).apply(int)\n",
    "            if col_name in models['model_2']:\n",
    "                models['model_2'].append(new_col)\n",
    "            models['model_3'].append(new_col)\n",
    "    zeros = dict(zip(columns, [0] * len(columns)))\n",
    "    df.fillna(zeros, inplace=True)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression\n",
    "\n",
    "There are 2 steps to this part.\n",
    "\n",
    "The first is to compute the mean and standard deviation for the control group for each dependant variable. This is done using `mean()` and `std()` on the control DataFrame (i.e. `vars_['treatment_var']` = False).\n",
    "\n",
    "The second part is to do a linear regression for each dependent variable once for each model. This is his done using 2 nested for loops (over models, then over dependent variables) and using `statsmodels`'s `OLS` with a cluster covariance estimator (`vars_['clustering_var']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to convert p-value to stars like in the paper\n",
    "def to_stars(p):\n",
    "    if p < 0.01:\n",
    "        return \"***\"\n",
    "    elif p < 0.05:\n",
    "        return \"** \"\n",
    "    elif p < 0.1:\n",
    "        return \"*  \"\n",
    "    return \"   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(df, models, vars_):\n",
    "    treatment_var = vars_['treatment_var'][0]\n",
    "    # Part 1: control\n",
    "    dependant_vars = vars_['dependant_vars']\n",
    "    control = df[df[treatment_var].apply(lambda x: not bool(x))][dependant_vars]\n",
    "    res = pd.DataFrame({\n",
    "        'control means': control.mean(),\n",
    "        'control std': control.std()\n",
    "    }, index=dependant_vars)\n",
    "    \n",
    "    # Part 2: linear regression\n",
    "    Y = df[dependant_vars]\n",
    "    \n",
    "    for k, v in models.items():\n",
    "        X = df[vars_['treatment_var'] + v]\n",
    "        X = sm.add_constant(X)\n",
    "        column = []\n",
    "        for label, y in Y.items():\n",
    "            regression = sm.OLS(y, X, missing='drop').fit(cov_type='cluster',\n",
    "                                          cov_kwds={'groups': df.dropna(subset=[label])[vars_['clustering_var']]})\n",
    "            coeff = regression.params[treatment_var]\n",
    "            significance = to_stars(regression.pvalues[treatment_var])\n",
    "            column.append((coeff, regression.bse[treatment_var], significance, \n",
    "                           100 * coeff / res.loc[label]['control means']))\n",
    "        res[k] = column\n",
    "    \n",
    "    return pd.DataFrame(res, index=dependant_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Showing and discussing the results\n",
    "\n",
    "We display the DataFrame containing the results rounded to 3 decimals (as in the originial paper), then compute the difference (after rounding) with the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round3(val):\n",
    "    if isinstance(val, nb.Number):\n",
    "        return round(val, 3)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        tpe = type(val)\n",
    "        return tpe(map(round3, val))\n",
    "\n",
    "def round_res(df, index):\n",
    "    res = df.apply(round3)\n",
    "    res.index = index\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y chars\n",
    "def char_compare(expected_star, real_star):\n",
    "    star_values = {'none':' ', 'star': '*'}\n",
    "    if(expected_star not in star_values.values() or real_star not in star_values.values()):\n",
    "        return '/'\n",
    "    is_expected_star = expected_star == star_values['star']\n",
    "    is_real_star = real_star == star_values['star']\n",
    "    res = [[' ', '+'], ['-', '*']]\n",
    "    return res[is_expected_star][is_real_star]\n",
    "\n",
    "def star_compare(expected_stars, real_stars):\n",
    "    return \"\".join(map(char_compare, list(expected_stars), list(real_stars)))\n",
    "\n",
    "def compare_results(expected_val, real_val):\n",
    "    if isinstance(expected_val, tuple):\n",
    "        return tuple(map(compare_results, expected_val, real_val))\n",
    "    elif not (isinstance(expected_val, str)):\n",
    "        return \"{:.2e}\".format(operator.sub(expected_val, real_val))\n",
    "    else:\n",
    "        return star_compare(expected_val, real_val)\n",
    "    \n",
    "def diff(expected_df, results_df):\n",
    "    comp = expected_df.copy()\n",
    "    for col in comp.columns:\n",
    "        comp[col] = list(map(compare_results, expected_df[col], results_df[col]))\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_output_results(df, models, vars_, expected_res):\n",
    "    res = compute_results(df, models, vars_)\n",
    "\n",
    "    rounded_res = round_res(res, vars_['dependant_vars'])\n",
    "    display(rounded_res)\n",
    "\n",
    "    display(expected_res)\n",
    "    comp = diff(expected_res, rounded_res)\n",
    "    display(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_Lp(p, control_row, treated_row):\n",
    "    if p == inf:\n",
    "        return max(abs(control_row-treated_row))\n",
    "    else:\n",
    "        return pow(sum(pow(abs(control_row-treated_row), p)), 1/p)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Helper function to compute a 1-to-1 matching depending on propensity scores\n",
    "# control_set: one of the dataframes on which we do the matching\n",
    "# treated_set: the second dataframe\n",
    "# p_distance: the p used for computing Lp distance\n",
    "# return: a 1-to-1 matching of the two dataframes minimizing the total difference of propensity scores\n",
    "def match(base_df, control_col, match_features, p_distance, epsilon=None):\n",
    "    G = nx.Graph()\n",
    "    filter_ = base_df[control_col].astype(bool).values\n",
    "    treated_set = base_df[filter_][match_features]\n",
    "    control_set = base_df.drop(treated_set.index)[match_features]\n",
    "    nodes_control, nodes_treat, distances = [], [], []\n",
    "    for control_id, control_row in tqdm(control_set.iterrows()):\n",
    "        for treat_id, treat_row in treated_set.iterrows():\n",
    "            nodes_control.append(control_id)\n",
    "            nodes_treat.append(treat_id)\n",
    "            distances.append(distance_Lp(p_distance, control_row, treat_row))\n",
    "    max_dist = max(distances)\n",
    "    weights = []\n",
    "    if epsilon:\n",
    "        popped = 0\n",
    "        for i in tqdm(range(len(distances))):\n",
    "            d = distances[i] / max_dist\n",
    "            if (d < epsilon):\n",
    "                nodes_control.pop(i-popped)\n",
    "                nodes_treat.pop(i-popped)\n",
    "                weights.append(1-d)\n",
    "                popped += 1\n",
    "    G.add_weighted_edges_from(zip(nodes_control, nodes_treat, weights))\n",
    "    print(len(G.edges))\n",
    "    \n",
    "    # max_weight_matching returns 1-to-1 matching\n",
    "    matches = nx.max_weight_matching(G)\n",
    "    \n",
    "    matched_df = pd.DataFrame(columns = base_df.columns)\n",
    "    for (l, r) in matches:\n",
    "        matched_df = matched_df.append(base_df.loc[l])\n",
    "        matched_df = matched_df.append(base_df.loc[r])\n",
    "    return matched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure reproduction: Table 4\n",
    "\n",
    "The goal of this part is to reproduce Table 4 of the paper _Housing, Health, and Happiness_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data and understanding what we'll need\n",
    "\n",
    "We start by identifying columns which contain our dependent variables, treatment variable, independent variables for each of the three models and clustering variable.\n",
    "\n",
    "The data related to the dependent variables can be fount in the following columns:\n",
    "  + Share of rooms with cement floors (`S_shcementfloor`)\n",
    "  + Cement floor in kitchen (`S_cementfloorkit`)\n",
    "  + Cement floor in dining room (`S_cementfloordin`)\n",
    "  + Cement floor in bathroom (`S_cementfloorbat`)\n",
    "  + Cement floor in bedroom (`S_cementfloorbed`)\n",
    "\n",
    "Control and treatment groups are identified by `dpisofirme` (control = 0, treatment = 1).\n",
    "\n",
    "Model 1 has no control variables.\n",
    "\n",
    "Model 2 has (25 - 1) control variables:\n",
    "  + demographic:\n",
    "    + Number of household members (`S_HHpeople`)\n",
    "    + (Number of rooms (`S_rooms`) -> This one is mentioned in the paper, but after looking at the STATA file, I noticed it was not used for the regression and decided to drop it)\n",
    "    + Head of household's years of schooling (`S_headeduc`)\n",
    "    + Spouse's years of schooling (`S_spouseeduc`)\n",
    "    + Head of household's age (`S_headage`)\n",
    "    + Spouse's age (`S_spouseage`)\n",
    "    + Proportion of Males 0-5yrs in household (`S_dem1`)\n",
    "    + Proportion of Males 6-17yrs in household (`S_dem2`)\n",
    "    + Proportion of Males 18-49yrs in household (`S_dem3`)\n",
    "    + Proportion of Males 50+yrs in household (`S_dem4`)\n",
    "    + Proportion of Females 0-5yrs in household (`S_dem5`)\n",
    "    + Proportion of Females 6-17yrs in household (`S_dem6`)\n",
    "    + Proportion of Females 18-49yrs in household (`S_dem7`)\n",
    "    + Proportion of Females 50+yrs in household (`S_dem8`)\n",
    "  + health:\n",
    "    + Household has animals on land (`S_hasanimals`)\n",
    "    + Animals allowed to enter the house (`S_animalsinside`)\n",
    "    + Water connection outside (`S_waterland`)\n",
    "    + Water connection inside the house (`S_waterhouse`)\n",
    "    + Electricity (`S_electricity`)\n",
    "    + Number of times respondent washed hands the day before (`S_washhands`)\n",
    "    + Uses garbage collection service (`S_garbage`)\n",
    "    \n",
    "Model 3 adds 4 control variables:\n",
    "  + Transfers per capita from government programs (`S_cashtransfers`)\n",
    "  + Household beneficiary of government milk supplement program (`S_milkprogram`)\n",
    "  + Household beneficiary of government food program (`S_foodprogram`)\n",
    "  + Household beneficiary of seguro popular (`S_seguropopular`)\n",
    "  \n",
    "All models use `idcluster` for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t4_full = pd.read_stata(DATA_PATH + \"PisoFirme_AEJPol-20070024_household.dta\")\n",
    "\n",
    "\"\"\"vars_t4 = {\n",
    "    \"treatment_var\": ['dpisofirme'],\n",
    "    \"clustering_var\": ['idcluster'],\n",
    "    \"demographic_control_vars\": ['S_HHpeople', 'S_headeduc', 'S_spouseeduc', 'S_headage',\n",
    "                                 'S_spouseage', 'S_dem1', 'S_dem2', 'S_dem3', 'S_dem4',\n",
    "                                 'S_dem5', 'S_dem6', 'S_dem7', 'S_dem8'],\n",
    "    \"health_control_vars\": ['S_hasanimals', 'S_animalsinside', 'S_waterland', 'S_waterhouse',\n",
    "                            'S_electricity', 'S_washhands', 'S_garbage'],\n",
    "    \"model3_control_vars\": ['S_cashtransfers', 'S_milkprogram', 'S_foodprogram', 'S_seguropopular'],\n",
    "    \"dependant_vars\": ['S_shcementfloor', 'S_cementfloorkit', 'S_cementfloordin',\n",
    "                       'S_cementfloorbat', 'S_cementfloorbed']\n",
    "}\n",
    "\n",
    "models_t4 = {\n",
    "    'model_1': [],\n",
    "    'model_2': vars_t4[\"demographic_control_vars\"] + vars_t4[\"health_control_vars\"],\n",
    "}\n",
    "models_t4['model_3'] = models_t4['model_2'] + vars_t4[\"model3_control_vars\"]\"\"\"\n",
    "vars_t4 = deepcopy(vars_)\n",
    "vars_t4[\"demographic_control_vars\"] = ['S_HHpeople', 'S_headeduc', 'S_spouseeduc', 'S_headage',\n",
    "                                       'S_spouseage', 'S_dem1', 'S_dem2', 'S_dem3', 'S_dem4',\n",
    "                                       'S_dem5', 'S_dem6', 'S_dem7', 'S_dem8']\n",
    "vars_t4[\"dependant_vars\"] = ['S_shcementfloor', 'S_cementfloorkit', 'S_cementfloordin',\n",
    "                             'S_cementfloorbat', 'S_cementfloorbed']\n",
    "models_t4 = generate_models(vars_t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_res_t4 = pd.DataFrame({'control means': [0.728, 0.671, 0.709, 0.803, 0.668],\n",
    "                             'control std': [0.363, 0.470, 0.455, 0.398, 0.471],\n",
    "                             'model_1': [(0.202, 0.021, '***', 27.746),\n",
    "                                         (0.255, 0.025, '***', 37.936),\n",
    "                                         (0.210, 0.026, '***', 29.633),\n",
    "                                         (0.105, 0.022, '***', 13.071),\n",
    "                                         (0.238, 0.020, '***', 35.598)],\n",
    "                             'model_2': [(0.208, 0.019, '***', 28.512),\n",
    "                                         (0.260, 0.023, '***', 38.708),\n",
    "                                         (0.217, 0.025, '***', 30.588),\n",
    "                                         (0.113, 0.018, '***', 14.043),\n",
    "                                         (0.245, 0.021, '***', 36.735)],\n",
    "                             'model_3':[(0.210, 0.019, '***', 28.876),\n",
    "                                        (0.265, 0.023, '***', 39.440),\n",
    "                                        (0.221, 0.025, '***', 31.189),\n",
    "                                        (0.117, 0.018, '***', 14.536),\n",
    "                                        (0.245, 0.020, '***', 36.695)]},\n",
    "                            index = vars_t4[\"dependant_vars\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data cleaning and generating missing values\n",
    "\n",
    "The next step is to clean the data, only keeping the necessary rows and generating missing values. \n",
    "\n",
    "Rows were identified as _unecessary_ if they were dropped in the original paper, which mentioned dropping samples for which geographical data was unavailable. Initially, we used that as a criteria for filtering the dataset. However, doing this gave worse results than not filtering at all. Thanks to a helpful comment on Zulip, we noticed a discreptancy between the number of samples we was supposed to have according to Table 1 (1362) and the number we had (1187). After looking around, we noticed geographical data was missing for the following 203 lines: (1788:1916), (2505:2527), (2576:2592), (2656:2661), (2755:2782) (see [Annex B1](#annexB1)). The last 28 lines seem to correspond to the 28 that were dropped in the paper, but the others seem to be due to some corruption of the dataset for some reason. Therefore, we decided to only drop the final 28 lines in order to use the same dataset as the paper, and we get much better results that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t4 = df_t4_full[:2755].copy()\n",
    "#df_t4 = df_t4[vars_t4[\"treatment_var\"] + vars_t4[\"clustering_var\"] + models_t4['model_3'] + vars_t4[\"dependant_vars\"]]\n",
    "new_models_t4 = generate_missing_values(df_t4, models_t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Computing and displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"res_t4 = compute_results(df_t4, new_models_t4, vars_t4)\n",
    "\n",
    "rounded_res_t4 = round_res(res_t4, vars_t4[\"dependant_vars\"])\n",
    "display(rounded_res_t4)\n",
    "\n",
    "display(expected_res_t4)\n",
    "comp_t4 = diff(expected_res_t4, rounded_res_t4)\n",
    "display(comp_t4)\"\"\"\n",
    "compute_and_output_results(df_t4, new_models_t4, vars_t4, expected_res_t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure reproduction: Table 5\n",
    "\n",
    "The goal of this milestone is to reproduce Table 5 of the paper _Housing, Health, and Happiness_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data and understanding what we'll need\n",
    "\n",
    "We started this replication exercise by adapting our replication of Table 4. The main differences are:\n",
    "1. we use the `individual.dta` rather than the `household.dta` file;\n",
    "1. we use different dependant variables.\n",
    "\n",
    "Note that Table 5 only focuses on children under the age of 6, so we drop every row pertaining to a person older than 6 years old (see [Annex A](#annexA) for further justification).\n",
    "\n",
    "Using the explanation in section V of the paper as well as the STATA code, we identified the columns which contain our dependent variables, treatment variable, independent variables for each of the three models and clustering variable.\n",
    "\n",
    "The data related to the dependant variables can be found in the following columns:\n",
    "  + Parasite count (`S_parcount`)\n",
    "  + Diarrhea (`S_diarrhea`)\n",
    "  + Anemia (`S_anemia`)\n",
    "  + McArthur Communication Development Test score (`S_mccdts`)\n",
    "  + Picture Peabody Vocabulary Test percentile score (`S_pbdypct`)\n",
    "  + Height-for-age z-score (`S_haz`)\n",
    "  + Weight-for-height z-score (`S_whz`)\n",
    "\n",
    "Control and treatment groups are identified by `dpisofirme` (control = 0, treatment = 1).\n",
    "\n",
    "Model 1 has no control variables.\n",
    "\n",
    "Model 2 has 58 control variables:\n",
    "  + demographic:\n",
    "    + Number of household members (`S_HHpeople`)\n",
    "    + Number of rooms (`S_rooms`)\n",
    "    + Age (`S_age`)\n",
    "    + Male (`S_gender`) -> the README specifies that Male = 1, but the loaded dataframe contained the values `0.0` and `hombre`, so this was corrected to be `0` and `1`\n",
    "    + Mother of at least one child in household present (`S_childma`)\n",
    "    + Mother's age (if present) (`S_childmaage`)\n",
    "    + Mother's years of schooling (if present) (`S_childmaeduc`)\n",
    "    + Father of at least one child in household present (`S_childpa`)\n",
    "    + Father's age (if present) (`S_childpaage`)\n",
    "    + Father's years of schooling (if present) (`S_childpaeduc`)\n",
    "    + (Trimester * Gender) Dummy for children 0-5yrs (`dtriage*`) [48]\n",
    "  + health:\n",
    "    + Household has animals on land (`S_hasanimals`)\n",
    "    + Animals allowed to enter the house (`S_animalsinside`)\n",
    "    + Water connection outside (`S_waterland`)\n",
    "    + Water connection inside the house (`S_waterhouse`)\n",
    "    + Electricity (`S_electricity`)\n",
    "    + Number of times respondent washed hands the day before (`S_washhands`)\n",
    "    + Uses garbage collection service (`S_garbage`)\n",
    "    \n",
    "Model 3 adds 4 control variables:\n",
    "  + Transfers per capita from government programs (`S_cashtransfers`)\n",
    "  + Household beneficiary of government milk supplement program (`S_milkprogram`)\n",
    "  + Household beneficiary of government food program (`S_foodprogram`)\n",
    "  + Household beneficiary of seguro popular (`S_seguropopular`)\n",
    "  \n",
    "All models use `idcluster` for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_t5 = pd.read_stata(DATA_PATH + \"PisoFirme_AEJPol-20070024_individual.dta\")\n",
    "\n",
    "\"\"\"vars_t5 = {\n",
    "    \"treatment_var\": ['dpisofirme'],\n",
    "    \"clustering_var\": ['idcluster'],\n",
    "    \"demographic_control_vars_1\": ['S_HHpeople', 'S_rooms', 'S_age', 'S_gender', 'S_childma', 'S_childmaage',\n",
    "                            'S_childmaeduc', 'S_childpa', 'S_childpaage', 'S_childpaeduc'],\n",
    "    \"demographic_control_vars_2\": [x for x in original_df_t5.columns if 'dtriage' in x],\n",
    "    \"health_control_vars\": ['S_hasanimals', 'S_animalsinside', 'S_waterland', 'S_waterhouse', \n",
    "                  'S_electricity', 'S_washhands', 'S_garbage'],\n",
    "    \"model3_control_vars\": ['S_cashtransfers', 'S_milkprogram', 'S_foodprogram', 'S_seguropopular'],\n",
    "    \"dependant_vars\": ['S_parcount', 'S_diarrhea', 'S_anemia', 'S_mccdts', 'S_pbdypct', 'S_haz', 'S_whz']\n",
    "}\n",
    "\n",
    "models_t5 = {\n",
    "    'model_1': [],\n",
    "    'model_2': vars_t5[\"demographic_control_vars_1\"] + vars_t5[\"demographic_control_vars_2\"] +\n",
    "                vars_t5[\"health_control_vars\"],\n",
    "}\n",
    "models_t5['model_3'] = models_t5['model_2'] + vars_t5[\"model3_control_vars\"]\"\"\"\n",
    "vars_t5 = deepcopy(vars_)\n",
    "demographic_control_vars_1 = ['S_HHpeople', 'S_rooms', 'S_age', 'S_gender', 'S_childma', 'S_childmaage',\n",
    "                              'S_childmaeduc', 'S_childpa', 'S_childpaage', 'S_childpaeduc']\n",
    "demographic_control_vars_2 = [x for x in original_df_t5.columns if 'dtriage' in x]\n",
    "vars_t5[\"demographic_control_vars\"] = demographic_control_vars_1 + demographic_control_vars_2\n",
    "vars_t5[\"dependant_vars\"] = ['S_parcount', 'S_diarrhea', 'S_anemia', 'S_mccdts', 'S_pbdypct', 'S_haz', 'S_whz']\n",
    "\n",
    "models_t5 = generate_models(vars_t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_res_t5 = pd.DataFrame({'control means': [0.333, 0.142, 0.426, 13.354, 30.656, -0.605, 0.125],\n",
    "                             'control std': [0.673, 0.349, 0.495, 18.952, 24.864, 1.104, 1.133],\n",
    "                             'model_1': [(-0.065, 0.032, '** ', -19.545),\n",
    "                                         (-0.018, 0.009, '*  ', -12.819),\n",
    "                                         (-0.085, 0.028, '***', -20.059), \n",
    "                                         (4.031, 1.650, '** ', 30.182), \n",
    "                                         (2.668, 1.689,'*  ' , 8.702), \n",
    "                                         (0.007, 0.043, '   ', -1.161), \n",
    "                                         (0.002, 0.034, '   ', 1.790)],\n",
    "                             'model_2': [(-0.064, 0.031, '** ', -19.345),\n",
    "                                         (-0.020, 0.009, '** ', -13.834),\n",
    "                                         (-0.081, 0.027, '***', -18.908),\n",
    "                                         (5.652, 1.642, '***', 42.325),\n",
    "                                         (3.206, 1.430, '** ', 10.460),\n",
    "                                         (0.002, 0.038, '   ',0.279),\n",
    "                                         (-0.005, 0.036, '   ', -4.119)],\n",
    "                             'model_3':[(-0.064, 0.032, '** ', -19.198),\n",
    "                                         (-0.018, 0.009, '*  ', -12.803),\n",
    "                                         (-0.083, 0.027, '***', -19.388),\n",
    "                                         (5.557, 1.641, '***', 41.609),\n",
    "                                         (3.083, 1.410, '** ', 10.058),\n",
    "                                         (-0.002, 0.039, '   ', -0.323),\n",
    "                                         (-0.011, 0.037, '   ', -8.727)]},\n",
    "                            index = vars_t5['dependant_vars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data cleaning and generating missing values\n",
    "\n",
    "The next step is to clean the data, only keeping the necessary rows and generating missing values.\n",
    "\n",
    "Once again, we identify clusters of missing geographical data in [Annex B2](#annexB2). However, the paper never mentions how many individuals' data they used, so we used the data provided in Table 1 for each dependant variable as a point of comparison. The check of the number of datapoints after dropping the final cluster is done in [Annex C](#annexC).\n",
    "\n",
    "Note: in the STATA file the paper's authors do not check for missing values in `dtriage` colums. Here, we confirm that there is no missing values in these columns, so we do not need to manually exclude them when generating missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_df_t5 = original_df_t5[original_df_t5.S_age < 6].reset_index(drop=True) # Magic number! See Annex B!\n",
    "children_df_t5['S_gender'] = children_df_t5['S_gender'].apply(lambda x: x == 'hombre').astype(int)\n",
    "df_t5 = children_df_t5[:4052].copy()\n",
    "\n",
    "#df_t5 = df_t5[vars_t5[\"treatment_var\"] + vars_t5[\"clustering_var\"] + vars_t5[\"dependant_vars\"] + models_t5['model_3'] + ['coord_x']]\n",
    "new_models_t5 = generate_missing_values(df_t5, models_t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Computing and displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"res_t5 = compute_results(df_t5, new_models_t5, vars_t5)\n",
    "\n",
    "rounded_res_t5 = round_res(res_t5, vars_t5['dependant_vars'])\n",
    "display(rounded_res_t5)\n",
    "\n",
    "display(expected_res_t5)\n",
    "comp_t5 = diff(expected_res_t5, rounded_res_t5)\n",
    "display(comp_t5)\"\"\"\n",
    "compute_and_output_results(df_t5, new_models_t5, vars_t5, expected_res_t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexA\"></span>\n",
    "### Annex A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars = []\n",
    "olderthan6_df = original_df_t5.drop(children_df_t5.index)[vars_t5[\"dependant_vars\"]]\n",
    "print('Checking if all dependant variables are `NaN` for individuals older than 6...')\n",
    "if (olderthan6_df.apply(pd.isna).values.all()):\n",
    "    s = \"\"\n",
    "else:\n",
    "    s = \"not\"\n",
    "print(f'We can{s} drop all aforementioned rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexB1\"></span>\n",
    "### Annex B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df_t4_full.loc[pd.isna(df_t4_full['coord_x'])].index\n",
    "ranges = []\n",
    "low, up = nans[0], nans[1]\n",
    "for i in range(len(nans) - 1):\n",
    "    up = nans[i]\n",
    "    if (up + 1 != nans[i+1] or i+1 == len(nans) - 1):\n",
    "        if (i+1 == len(nans) - 1):\n",
    "            up = nans[i+1]\n",
    "        ranges.append((low, up))\n",
    "        low = nans[i+1]\n",
    "ranges_str = \"\"\n",
    "for (l, r) in ranges:\n",
    "    ranges_str += '[' + str(l) + ', ' +  str(r) + ']\\n'\n",
    "print(f'Intervals of rows with missing geographical data:\\n{ranges_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexB2\"></span>\n",
    "### Annex B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = children_df_t5.loc[pd.isna(children_df_t5['coord_x'])].index\n",
    "ranges = []\n",
    "low, up = nans[0], nans[1]\n",
    "for i in range(len(nans) - 1):\n",
    "    up = nans[i]\n",
    "    if (up + 1 != nans[i+1] or i+1 == len(nans) - 1):\n",
    "        if (i+1 == len(nans) - 1):\n",
    "            up = nans[i+1]\n",
    "        ranges.append((low, up))\n",
    "        low = nans[i+1]\n",
    "ranges_str = \"\"\n",
    "for (l, r) in ranges:\n",
    "    ranges_str += '[' + str(l) + ', ' +  str(r) + ']\\n'\n",
    "print(f'Intervals of rows with missing geographical data:\\n{ranges_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"annexC\"></span>\n",
    "### Annex C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_amount = pd.DataFrame({'treat_expected': [1528, 1930, 1768, 291, 757, 1865, 1881],\n",
    "                                'cont_expected' : [1566, 2105, 1951, 302, 817, 2053, 2058]},\n",
    "                              index=vars_t5[\"dependant_vars\"])\n",
    "treat, cont, drop_t, drop_c = [], [], [], []\n",
    "treat_df = df_t5[df_t5.dpisofirme == 1]\n",
    "cont_df = df_t5[df_t5.dpisofirme == 0]\n",
    "\n",
    "for col in vars_t5[\"dependant_vars\"]:\n",
    "    dt = len(treat_df.loc[pd.isna(treat_df[col])])\n",
    "    treat.append(len(treat_df) - dt)\n",
    "    drop_t.append(dt)\n",
    "    dc = len(cont_df.loc[pd.isna(cont_df[col])])\n",
    "    cont.append(len(cont_df) - dc)\n",
    "    drop_c.append(dc)\n",
    "expected_amount['treat'] = treat\n",
    "expected_amount['cont'] = cont\n",
    "expected_amount['dropped_treat'] = drop_t\n",
    "expected_amount['dropped_cont'] = drop_c\n",
    "expected_amount['dropped_tot'] = expected_amount['dropped_treat'] + expected_amount['dropped_cont']\n",
    "expected_amount['delta_t'] = expected_amount.treat_expected - expected_amount.treat\n",
    "expected_amount['delta_c'] = expected_amount.cont_expected - expected_amount.cont\n",
    "expected_amount['tot'] = expected_amount.treat + expected_amount.cont\n",
    "expected_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t4_matched = match(df_t4, vars_t4['treatment_var'], [\"S_incomepc\", \"S_assetspc\", \"S_shpeoplework\"], inf, epsilon=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_t4_matched = compute_results(df_t4_matched.reset_index(drop=True), new_models_t4, vars_t4)\n",
    "\n",
    "rounded_res_t4_matched = round_res(res_t4_matched, vars_t4[\"dependant_vars\"])\n",
    "display(rounded_res_t4_matched)\n",
    "\n",
    "display(expected_res_t4)\n",
    "comp_t4 = diff(expected_res_t4, rounded_res_t4_matched)\n",
    "display(comp_t4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
